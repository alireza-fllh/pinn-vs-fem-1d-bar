# ğŸ—ï¸ # PINN vs FEM for a 1D Elastic Bar

**Physics-Informed Neural Networks (PINNs) vs a purely data-driven black-box** on a classical solid-mechanics benchmark: the axial displacement of a prismatic bar. The repo provides:
- ğŸ”§ A minimal **FEM reference solver**,
- ğŸ§  A **PINN** that enforces the PDE and boundary conditions,
- ğŸ“Š A **black-box MLP** trained on noisy supervised data,
- ğŸ”¬ Reproducible **experiments** (data-efficiency, noise robustness, extrapolation).

## ğŸ“ Problem statement

Consider a straight 1D bar of length $\text(L)$ with axial displacement $u(x)$. Let $E(x)$ be Young's modulus, $A(x)$ crossâ€‘sectional area, and $f(x)$ the axial body force per unit length. The governing equation in strong form is:


$\Large{-\frac{d}{dx}\left(E(x)A(x)\frac{du}{dx}\right) = f(x), \quad x\in(0,L).}$

Supported boundary conditions on $x=0$ and $x=L$:

- ğŸ“Œ **Dirichlet (displacement)**: $u=u_0$.
- âš¡ **Neumann (traction / tip load)**: $EA\,u'(x)=P$.
- ğŸ”— **Robin (spring / convective)**: $EA\,u'(x)+h\,u = g$.

> ğŸ’¡ This project mainly showcases **tip load** and **body force** cases with homogeneous material; heterogeneity and Robin BCs are included in the code and easy to toggle.lastic Bar

---


## ğŸ“‚ Repository layout

    .
    â”œâ”€â”€ ğŸ¨ assets/
    â”œâ”€â”€ ğŸ“ data/
    â”‚   â”œâ”€â”€ ğŸ“Š outputs/                # FEM CSVs, logs, figures
    â”‚   â””â”€â”€ ğŸ¯ supervised/             # generated BB datasets
    â”œâ”€â”€ ğŸ’» src/
    â”‚   â”œâ”€â”€ âš™ï¸ core/                   # reusable components
    â”‚   â”‚   â”œâ”€â”€ fem.py
    â”‚   â”‚   â”œâ”€â”€ pinn.py
    â”‚   â”‚   â”œâ”€â”€ physics.py
    â”‚   â”‚   â””â”€â”€ utils.py
    â”‚   â”œâ”€â”€ ğŸš€ training/
    â”‚   â”‚   â”œâ”€â”€ run_fem.py             # CLI: make FEM CSV for a case/BCs
    â”‚   â”‚   â”œâ”€â”€ run_pinn.py            # CLI: train PINN & save log
    â”‚   â”‚   â””â”€â”€ train_blackbox.py      # CLI: train supervised MLP & save log
    â”‚   â”œâ”€â”€ ğŸ”¬ experiments/
    â”‚   â”‚   â”œâ”€â”€ data_gen.py            # create noisy datasets for the Black box model
    â”‚   â”‚   â””â”€â”€ sweep_metrics.py       # data-efficiency & noise robustness sweeps
    â”‚   â””â”€â”€ ğŸ“ˆ visualization/
    â”‚       â”œâ”€â”€ plot_metrics.py
    â”‚       â””â”€â”€ hero_figure.py
    â”œâ”€â”€ ğŸ§ª tests/
    â”œâ”€â”€ âš¡ Makefile                    # one-command pipelines
    â”œâ”€â”€ ğŸŒ env.yml
    â”œâ”€â”€ ğŸ“¦ pyproject.toml              # packaging metadata
    â””â”€â”€ ğŸ“– README.md                   # this file

---

## âš¡ Quickstart (60 seconds)

```bash
    # 0ï¸âƒ£ Create env
    conda env create -f env.yml
    conda activate py310-torch

    # 1ï¸âƒ£ In-range demo (tip load P=0.60) â†’ FEM + PINN + BB + joint figure
    make inrange CASE=tip_load P=0.60 EPOCHS=1200 BB_EPOCHS=1200

    # 2ï¸âƒ£ Extrapolation demo (outside BB training range)
    make extrap               # alias for P=1.20; adjust in Makefile if desired

    # 3ï¸âƒ£ Generate the hero figure
    make hero
  ```

ğŸ“ Results appear under `data/outputs/<CASE>_P<P>/`. Example artifacts:
- `fem_tip_load.csv` â€“ reference solution $(x,u)$
- `pinn_tip_load_trainlog.npz` â€“ PINN losses & snapshots,
- `bb_trainlog.npz` â€“ black-box losses & snapshots,
- `hero.png/.svg`

## What's implemented
- **FEM reference** with linear 1D bar elements.
- **PINN**:
  - fully connected MLP
  - PDE residual + weighted BC losses
  - optional input/output normalization.
- **Black-box MLP**: supervised $[x, P] \rightarrow u$, integrated no pysics.
- **Experiments**:
  - *Data efficiency*: error vs number of samples per configuration $(M)$
  - *Noise robustness*: error vs label noise $\sigma$
  - *Reliability*: $\mu \displaystyle \pm \sigma$ across seeds.
  - *Extrapolation*: evaluation at $P$ beyond dataset range

## ğŸ† Selected results

<p align="center">
  <img src="data/outputs/hero_full.png" width="820" alt="Hero figure â€” tip load (in-range & extrapolation)">
</p>

<p align="center">
  <img src="data/outputs/anim_joint_extrap.gif" width="820" alt="Joint animation â€” predictions (top) + losses (bottom), tip load extrapolation P=1.20">
  <br/>
  <em></em>
</p>

### ğŸ“Š Key Findings:

- âœ… For **interpolation** (in-range), $\text{PINN}$ and $\text{BB}$ achieve low error; $\text{PINN}$ converges in fewer epochs.
- ğŸš€ For **Extrapolation**, $\text{PINN}$ remains accurate; $\text{BB}$ deteriorates.
- ğŸ›¡ï¸ In the case of **noisy data**, $\text{PINN}$ has higher reliability.
- ğŸ“ˆ On a **limited amount of training data** samples, $\text{PINN}$ functions much better than $\text{BB}$.

